# ğŸ“˜ predictions.py - Complete Line-by-Line Explanation

## ğŸ“‹ Table of Contents
1. [File Header & Imports](#file-header--imports)
2. [Educational Keywords Database](#educational-keywords-database)
3. [Year Extraction Functions](#year-extraction-functions)
4. [Clustering Functions](#clustering-functions)
5. [Forecasting Functions](#forecasting-functions)
6. [Detection Functions](#detection-functions)
7. [ML Training Functions](#ml-training-functions)

---

## 1. File Header & Imports (Lines 1-22)

### Lines 1-7: File Documentation
```python
# predictions.py - Complete ML Predictions with Clustering & Batch Processing
# âœ” Detects & labels columns automatically
# âœ” Clusters numeric, categorical, and year-based columns
# âœ” Handles educational and general datasets
# âœ” Performs automatic ARIMA forecasting (with regression fallback)
# âœ” Trains RandomForest models for regression/classification
# âœ” Returns detailed mappings, forecasts, and metrics
```

**What it does:** Header comment describing file purpose and capabilities

**Why:** Documentation for developers to understand file scope

---

### Lines 9-11: Core Libraries
```python
import re
import pandas as pd
import numpy as np
```

**Line-by-line:**
- `import re`: Regular expressions for pattern matching (year extraction)
- `import pandas as pd`: Data manipulation library (DataFrames)
- `import numpy as np`: Numerical computing (arrays, math operations)

**Usage in file:**
- `re`: Used in `_YEAR_REGEX` to find years like "2023" in text
- `pandas`: Used everywhere for data processing
- `numpy`: Used for numerical operations in forecasting

---

### Line 12: Type Hints
```python
from typing import Dict, Any, Optional, Tuple, List
```

**What it does:** Imports type hint classes for function signatures

**Why:** Makes code more readable and enables IDE autocomplete

**Example:**
```python
def my_function(name: str) -> Dict[str, Any]:
    # name must be string, returns dictionary
```

---

### Lines 13-16: Machine Learning Libraries
```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, accuracy_score
```

**Line-by-line:**
- `train_test_split`: Splits data into training and testing sets
- `RandomForestClassifier`: ML model for classification (categories)
- `RandomForestRegressor`: ML model for regression (numbers)
- `LinearRegression`: Simple linear model for forecasting
- `mean_squared_error`: Measures prediction error (lower = better)
- `accuracy_score`: Measures classification accuracy (higher = better)

**Usage:**
- Used in `train_and_predict()` function
- Used in `simple_forecast_model()` function

---

### Lines 17-18: Time Series Libraries
```python
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
```

**Line-by-line:**
- `ARIMA`: AutoRegressive Integrated Moving Average model for forecasting
- `adfuller`: Augmented Dickey-Fuller test (checks if data is stationary)

**Usage:**
- Used in `arima_forecast_students()` function
- Tests if data has trend/seasonality

---

### Lines 19-21: Utilities
```python
import math
import warnings
warnings.filterwarnings('ignore')
```

**Line-by-line:**
- `import math`: Mathematical functions (not heavily used)
- `import warnings`: Python warning system
- `warnings.filterwarnings('ignore')`: Suppresses warning messages

**Why suppress warnings:**
- ARIMA can generate many convergence warnings
- Keeps output clean for users
- Warnings don't affect functionality

---

### Line 24: Year Regex Pattern
```python
_YEAR_REGEX = re.compile(r'\b(19|20)\d{2}\b')
```

**What it does:** Creates regex pattern to match 4-digit years

**Pattern breakdown:**
- `\b`: Word boundary (start of word)
- `(19|20)`: Matches "19" OR "20"
- `\d{2}`: Matches exactly 2 digits
- `\b`: Word boundary (end of word)

**Matches:** 1900-2099 (e.g., "2023", "1995")
**Doesn't match:** "123", "3000", "202" (wrong format)

**Example:**
```python
text = "Enrollment in 2023/24 was 1000"
match = _YEAR_REGEX.search(text)
print(match.group(0))  # Output: "2023"
```

---

## 2. Educational Keywords Database (Lines 27-130)

### Lines 27-28: Dictionary Declaration
```python
EDUCATIONAL_KEYWORDS = {
    # K-12 Core Subjects
```

**What it does:** Creates dictionary to store educational keywords by category

**Structure:**
```python
{
    'category_name': ['keyword1', 'keyword2', ...],
    'another_category': ['keyword3', 'keyword4', ...]
}
```

---

### Lines 30-35: K-12 Subjects
```python
'k12_subjects': ['mathematics', 'math', 'algebra', 'geometry', 'calculus', 'statistics', 'trigonometry',
                 'science', 'biology', 'chemistry', 'physics', 'earth science', 'environmental science',
                 'english', 'language arts', 'literature', 'reading', 'writing', 'composition',
                 'social studies', 'history', 'geography', 'civics', 'government', 'economics',
                 'arts', 'music', 'visual art', 'drama', 'theater', 'dance',
                 'physical education', 'health', 'pe', 'stem', 'steam'],
```

**What it does:** Lists all K-12 school subjects

**Usage:** Detects if dataset contains school-related data

**Example:**
```python
column_name = "Mathematics_Score"
if 'mathematics' in column_name.lower():
    print("This is a K-12 subject!")
```

---

### Lines 37-44: STEM & Natural Sciences
```python
'stem_sciences': ['computer science', 'cs', 'information technology', 'it', 'programming',
                  'data science', 'artificial intelligence', 'machine learning',
                  'astronomy', 'astrophysics', 'biochemistry', 'biophysics', 'biotechnology',
                  'molecular biology', 'genetics', 'microbiology', 'zoology', 'botany',
                  'organic chemistry', 'inorganic chemistry', 'analytical chemistry',
                  'quantum physics', 'nuclear physics', 'applied physics',
                  'pure mathematics', 'applied mathematics', 'discrete mathematics'],
```

**What it does:** Lists university-level STEM subjects

**Coverage:** Computer Science, Biology, Chemistry, Physics, Mathematics

---

### Lines 46-53: Engineering Branches
```python
'engineering': ['engineering', 'chemical engineering', 'civil engineering', 'structural engineering',
                'electrical engineering', 'electronic engineering', 'mechanical engineering',
                'aerospace engineering', 'aeronautical engineering', 'automotive engineering',
                'biomedical engineering', 'industrial engineering', 'manufacturing engineering',
                'materials engineering', 'metallurgical engineering', 'mining engineering',
                'petroleum engineering', 'software engineering', 'systems engineering',
                'environmental engineering', 'agricultural engineering', 'marine engineering',
                'robotics', 'mechatronics', 'nanotechnology'],
```

**What it does:** Lists all major engineering disciplines

**Coverage:** 20+ engineering branches from Chemical to Software

---

### Lines 55-64: Medical & Health Sciences
```python
'medical_health': ['medicine', 'medical', 'nursing', 'dentistry', 'dental', 'pharmacy', 'pharmaceutical',
                   'public health', 'health sciences', 'biomedical sciences', 'clinical sciences',
                   'anesthesiology', 'cardiology', 'dermatology', 'emergency medicine',
                   'endocrinology', 'gastroenterology', 'hematology', 'immunology',
                   'neurology', 'neuroscience', 'obstetrics', 'gynecology', 'oncology',
                   'ophthalmology', 'orthopedics', 'otolaryngology', 'pathology',
                   'pediatrics', 'psychiatry', 'radiology', 'surgery', 'urology',
                   'epidemiology', 'anatomy', 'physiology', 'pharmacology', 'toxicology',
                   'physical therapy', 'occupational therapy', 'medical laboratory',
                   'radiography', 'respiratory therapy', 'nutrition', 'dietetics'],
```

**What it does:** Lists medical specialties and health sciences

**Coverage:** 
- Medical specialties (Cardiology, Neurology, etc.)
- Allied health (Nursing, Physical Therapy, etc.)
- Basic sciences (Anatomy, Physiology, etc.)

---

### Lines 66-73: Business, Finance & Economics
```python
'business_economics': ['business', 'business administration', 'management', 'mba',
                       'accounting', 'finance', 'economics', 'econometrics',
                       'marketing', 'supply chain', 'operations', 'logistics',
                       'human resources', 'hr', 'entrepreneurship', 'strategy',
                       'hospitality', 'tourism', 'retail', 'e-commerce',
                       'international business', 'organizational behavior'],
```

**What it does:** Lists business and economics fields

**Coverage:** Management, Finance, Marketing, HR, Operations

---

### Lines 75-82: Humanities
```python
'humanities': ['english literature', 'comparative literature', 'creative writing',
               'history', 'ancient history', 'modern history', 'art history',
               'philosophy', 'ethics', 'logic', 'metaphysics',
               'religious studies', 'theology', 'divinity',
               'languages', 'linguistics', 'spanish', 'french', 'german', 'chinese',
               'japanese', 'arabic', 'latin', 'greek', 'italian', 'russian',
               'fine arts', 'performing arts', 'film studies', 'media studies'],
```

**What it does:** Lists humanities subjects

**Coverage:** Literature, History, Philosophy, Languages, Arts

---

### Lines 84-90: Social Sciences
```python
'social_sciences': ['psychology', 'clinical psychology', 'cognitive psychology',
                    'sociology', 'anthropology', 'archaeology',
                    'political science', 'international relations', 'public policy',
                    'criminology', 'criminal justice', 'social work',
                    'human geography', 'urban planning', 'demography',
                    'communication', 'journalism', 'public relations'],
```

**What it does:** Lists social science disciplines

**Coverage:** Psychology, Sociology, Political Science, etc.

---

### Lines 92-97: Education & Teaching
```python
'education': ['education', 'teacher education', 'curriculum', 'instruction',
              'pedagogy', 'educational psychology', 'special education',
              'early childhood education', 'elementary education', 'secondary education',
              'higher education', 'adult education', 'distance learning',
              'educational technology', 'instructional design'],
```

**What it does:** Lists education-related fields

**Coverage:** Teaching, Curriculum, Educational Technology

---

### Lines 99-106: Professional & Applied Fields
```python
'professional': ['law', 'legal studies', 'jurisprudence',
                 'architecture', 'urban design', 'landscape architecture',
                 'library science', 'information science',
                 'kinesiology', 'sports science', 'exercise science',
                 'agriculture', 'agronomy', 'horticulture', 'veterinary',
                 'forestry', 'fisheries', 'food science',
                 'design', 'graphic design', 'fashion design', 'interior design'],
```

**What it does:** Lists professional and applied fields

**Coverage:** Law, Architecture, Agriculture, Design

---

### Lines 108-116: Educational Metrics
```python
'metrics': ['enrollment', 'enrolment', 'students', 'pupils', 'learners',
            'attendance', 'graduation', 'dropout', 'retention',
            'gpa', 'grade', 'score', 'marks', 'test', 'exam', 'assessment',
            'faculty', 'staff', 'teachers', 'professors', 'instructors',
            'tuition', 'fees', 'scholarship', 'financial aid',
            'admissions', 'applications', 'acceptance rate',
            'class size', 'student-teacher ratio', 'credits', 'courses'],
```

**What it does:** Lists common educational metrics and measurements

**Usage:** Identifies columns containing educational statistics

**Example:**
```python
column = "Student_Enrollment_2023"
if 'enrollment' in column.lower():
    print("This is an educational metric!")
```

---

### Lines 118-123: Institution Types
```python
'institutions': ['school', 'college', 'university', 'institute', 'academy',
                 'elementary', 'primary', 'secondary', 'high school',
                 'undergraduate', 'graduate', 'postgraduate', 'doctoral',
                 'campus', 'department', 'faculty', 'division', 'program']
```

**What it does:** Lists types of educational institutions

**Coverage:** Schools, Colleges, Universities, Academic levels

---

### Lines 127-130: Flatten Keywords
```python
# Flatten all keywords for quick lookup
ALL_EDUCATIONAL_KEYWORDS = set()
for category, keywords in EDUCATIONAL_KEYWORDS.items():
    ALL_EDUCATIONAL_KEYWORDS.update([k.lower() for k in keywords])
```

**What it does:** Creates single set of all keywords for fast searching

**Line-by-line:**
1. `ALL_EDUCATIONAL_KEYWORDS = set()`: Create empty set
2. `for category, keywords in EDUCATIONAL_KEYWORDS.items()`: Loop through each category
3. `ALL_EDUCATIONAL_KEYWORDS.update([k.lower() for k in keywords])`: Add all keywords (lowercase)

**Result:**
```python
ALL_EDUCATIONAL_KEYWORDS = {
    'mathematics', 'math', 'algebra', 'engineering', 'medicine', ...
    # 250+ keywords in one flat set
}
```

**Why flatten:**
- Faster lookup: `if keyword in ALL_EDUCATIONAL_KEYWORDS` is O(1)
- Don't need to loop through categories
- Lowercase for case-insensitive matching

---

## 3. Year Extraction Functions (Lines 132-145)

### Lines 132-143: extract_year_from_string()
```python
def extract_year_from_string(s: str) -> Optional[int]:
    """Extract first 4-digit year from string."""
    if pd.isna(s):
        return None
    s = str(s)
    m = _YEAR_REGEX.search(s)
    if m:
        try:
            return int(m.group(0))
        except Exception:
            return None
    return None
```

**Line-by-line explanation:**

**Line 132:** Function signature
- `s: str`: Input parameter (string to search)
- `-> Optional[int]`: Returns integer or None

**Line 134:** Check if value is missing
- `pd.isna(s)`: Pandas function to check for NaN/None
- Returns `None` if missing

**Line 136:** Convert to string
- `s = str(s)`: Ensures input is string (handles numbers)
- Example: `2023` â†’ `"2023"`

**Line 137:** Search for year pattern
- `m = _YEAR_REGEX.search(s)`: Finds first year match
- Returns Match object or None

**Lines 138-142:** Extract and return year
- `if m:`: If match found
- `return int(m.group(0))`: Convert match to integer
- `except Exception:`: Handle conversion errors
- `return None`: If no match or error

**Examples:**
```python
extract_year_from_string("2023/24")      # Returns: 2023
extract_year_from_string("Year 2020")    # Returns: 2020
extract_year_from_string("Hello")        # Returns: None
extract_year_from_string(2023)           # Returns: 2023
extract_year_from_string(None)           # Returns: None
```

---

### Lines 145-148: round_years_in_column()
```python
def round_years_in_column(series: pd.Series) -> pd.Series:
    """Convert Series to integer years. Examples: '2000/01' -> 2000"""
    extracted = series.astype(str).apply(lambda x: extract_year_from_string(x))
    return pd.Series(extracted, index=series.index, dtype="Int64")
```

**Line-by-line explanation:**

**Line 145:** Function signature
- `series: pd.Series`: Input pandas Series (column)
- `-> pd.Series`: Returns pandas Series

**Line 147:** Extract years from all values
- `series.astype(str)`: Convert all values to strings
- `.apply(lambda x: extract_year_from_string(x))`: Apply extraction to each value
- `lambda x:`: Anonymous function for each element

**Line 148:** Return as Series
- `pd.Series(extracted, ...)`: Create new Series
- `index=series.index`: Keep original row indices
- `dtype="Int64"`: Integer type that allows NaN

**Example:**
```python
Input Series:
0    2020/21
1    2021/22
2    2022/23
3    Invalid

Output Series:
0    2020
1    2021
2    2022
3    NaN
```

**Why this function:**
- Handles various year formats ("2020", "2020/21", "2020-2021")
- Extracts first year from academic year ranges
- Converts to clean integer format

---

## 4. Clustering Functions (Lines 150-240)

### Lines 150-167: cluster_numeric_column()
```python
def cluster_numeric_column(series: pd.Series, n_bins: int = 8) -> pd.Series:
    """Cluster numeric values into bins."""
    s = series.dropna()
    if s.empty or len(s) < 2:
        return pd.Series(["unknown"] * len(series), index=series.index, dtype="object")

    try:
        if s.nunique() > n_bins:
            bins = pd.qcut(s, q=n_bins, duplicates="drop")
        else:
            bins = pd.cut(s, bins=min(n_bins, s.nunique()))

        labels = pd.Series("unknown", index=series.index, dtype="object")
        labels.loc[bins.index] = bins.astype(str)
        return labels
    except Exception:
        return pd.Series(["unknown"] * len(series), index=series.index, dtype="object")
```

**Line-by-line explanation:**

**Line 150:** Function signature
- `series: pd.Series`: Numeric column to cluster
- `n_bins: int = 8`: Number of bins (default 8)
- `-> pd.Series`: Returns clustered labels

**Line 152:** Remove missing values
- `s = series.dropna()`: Creates copy without NaN
- Needed for binning algorithms

**Lines 153-154:** Check if enough data
- `if s.empty or len(s) < 2:`: Need at least 2 values
- Returns "unknown" for all if insufficient data

**Lines 156-160:** Choose binning method
- `if s.nunique() > n_bins:`: If more unique values than bins
  - `pd.qcut(s, q=n_bins, duplicates="drop")`: Quantile-based binning
  - Equal number of items per bin
- `else:`: If few unique values
  - `pd.cut(s, bins=min(n_bins, s.nunique()))`: Range-based binning
  - Equal width bins

**Lines 162-164:** Create labeled series
- `labels = pd.Series("unknown", ...)`: Initialize all as "unknown"
- `labels.loc[bins.index] = bins.astype(str)`: Fill in bin labels
- Preserves original indices

**Lines 165-166:** Error handling
- `except Exception:`: Catch any binning errors
- Returns "unknown" for all values

**Example:**
```python
Input: [18, 19, 20, 25, 30, 35, 40, 45]

Output with qcut (n_bins=3):
["(17.999, 20.667]", "(17.999, 20.667]", "(17.999, 20.667]",
 "(20.667, 32.5]", "(20.667, 32.5]", "(32.5, 45.0]",
 "(32.5, 45.0]", "(32.5, 45.0]"]
```

---

### Lines 169-173: cluster_categorical_column()
```python
def cluster_categorical_column(series: pd.Series, top_k: int = 15) -> pd.Series:
    """Keep top_k categories, group others as 'Other'."""
    s = series.fillna("MISSING").astype(str)
    top_categories = s.value_counts().nlargest(top_k).index.tolist()
    clustered = s.apply(lambda x: x if x in top_categories else "Other")
    return clustered
```

**Line-by-line explanation:**

**Line 169:** Function signature
- `series: pd.Series`: Categorical column
- `top_k: int = 15`: Keep top 15 categories
- `-> pd.Series`: Returns clustered categories

**Line 171:** Prepare data
- `series.fillna("MISSING")`: Replace NaN with "MISSING"
- `.astype(str)`: Convert all to strings

**Line 172:** Find top categories
- `s.value_counts()`: Count occurrences of each category
- `.nlargest(top_k)`: Get top K most frequent
- `.index.tolist()`: Extract category names as list

**Line 173:** Group rare categories
- `s.apply(lambda x: ...)`: Apply to each value
- `x if x in top_categories else "Other"`: Keep if top, else "Other"

**Example:**
```python
Input (top_k=3):
["USA", "UK", "USA", "Canada", "France", "USA", "Germany", "UK"]

Value counts:
USA: 3, UK: 2, Canada: 1, France: 1, Germany: 1

Output:
["USA", "UK", "USA", "Other", "Other", "USA", "Other", "UK"]
```

**Why this function:**
- Reduces cardinality (fewer unique values)
- Prevents overfitting on rare categories
- Improves ML model performance

---

*[Continuing in next part due to length...]*


### Lines 175-240: cluster_all_columns()
```python
def cluster_all_columns(
    df: pd.DataFrame,
    numeric_bins: int = 8,
    cat_top_k: int = 15,
    treat_years: bool = True,
    keep_original: bool = True
) -> Tuple[pd.DataFrame, Dict[str, Dict[str, Any]]]:
    """Apply intelligent clustering to ALL columns."""
    df_proc = df.copy()
    mappings = {}

    for col in df.columns:
        col_series = df[col]

        # 1) Year Detection & Clustering
        if treat_years:
            name_hints = any(hint in col.lower() for hint in ["year", "date", "period", "time"])
            sample_vals = col_series.dropna().astype(str).head(30).tolist()
            value_hints = sum(1 for s in sample_vals if _YEAR_REGEX.search(s))

            if name_hints or value_hints >= len(sample_vals) // 4:
                year_series = round_years_in_column(col_series)
                if year_series.notna().sum() >= max(3, len(col_series) // 10):
                    cluster_col = f"{col}__year" if keep_original else col
                    df_proc[cluster_col] = year_series

                    unique_years = sorted([int(x) for x in year_series.dropna().unique()])
                    mappings[col] = {
                        "type": "year",
                        "cluster_col": cluster_col,
                        "unique_clusters": unique_years,
                        "description": f"Year clustering: {len(unique_years)} unique years"
                    }
                    continue

        # 2) Numeric Clustering
        if pd.api.types.is_numeric_dtype(col_series):
            cluster_col = f"{col}__bin" if keep_original else col
            labels = cluster_numeric_column(col_series, n_bins=numeric_bins)
            df_proc[cluster_col] = labels

            unique_bins = labels.dropna().unique().tolist()
            mappings[col] = {
                "type": "numeric",
                "cluster_col": cluster_col,
                "unique_clusters": unique_bins,
                "description": f"Numeric binning: {len(unique_bins)} bins"
            }
            continue

        # 3) Categorical Clustering
        cluster_col = f"{col}__cat" if keep_original else col
        labels = cluster_categorical_column(col_series, top_k=cat_top_k)
        df_proc[cluster_col] = labels

        unique_cats = labels.unique().tolist()
        mappings[col] = {
            "type": "categorical", 
            "cluster_col": cluster_col,
            "unique_clusters": unique_cats,
            "description": f"Categorical grouping: {len(unique_cats)} groups"
        }

    return df_proc, mappings
```

**MASTER CLUSTERING FUNCTION - Detailed Explanation:**

**Lines 175-181:** Function signature and parameters
- `df: pd.DataFrame`: Input dataset
- `numeric_bins: int = 8`: Number of bins for numeric columns
- `cat_top_k: int = 15`: Keep top 15 categories
- `treat_years: bool = True`: Enable year detection
- `keep_original: bool = True`: Keep original columns
- Returns: (processed_df, mappings_dict)

**Lines 183-184:** Initialize
- `df_proc = df.copy()`: Create copy to avoid modifying original
- `mappings = {}`: Store clustering information

**Line 186:** Loop through all columns
- `for col in df.columns:`: Process each column

**Lines 190-207: YEAR DETECTION & CLUSTERING**

**Line 191:** Check if year detection enabled
- `if treat_years:`: Only if parameter is True

**Line 192:** Check column name for year hints
- `name_hints = any(hint in col.lower() for hint in ["year", "date", "period", "time"])`
- Checks if column name contains time-related words
- Example: "Academic_Year" â†’ True, "Student_Name" â†’ False

**Lines 193-194:** Check column values for years
- `sample_vals = col_series.dropna().astype(str).head(30).tolist()`: Get first 30 values
- `value_hints = sum(1 for s in sample_vals if _YEAR_REGEX.search(s))`: Count how many contain years
- Example: ["2020", "2021", "2022"] â†’ value_hints = 3

**Line 196:** Decide if column is year-based
- `if name_hints or value_hints >= len(sample_vals) // 4:`
- True if: column name suggests years OR 25%+ of values are years

**Line 197:** Extract years
- `year_series = round_years_in_column(col_series)`: Convert to integer years

**Line 198:** Validate enough year data
- `if year_series.notna().sum() >= max(3, len(col_series) // 10):`
- Need at least 3 years OR 10% of rows with valid years

**Lines 199-200:** Create new column
- `cluster_col = f"{col}__year" if keep_original else col`
- If keep_original=True: "Academic_Year" â†’ "Academic_Year__year"
- If keep_original=False: "Academic_Year" â†’ "Academic_Year" (replace)
- `df_proc[cluster_col] = year_series`: Add to dataframe

**Lines 202-207:** Store mapping information
- `unique_years = sorted([int(x) for x in year_series.dropna().unique()])`: Get sorted list of years
- `mappings[col] = {...}`: Store metadata
  - `"type": "year"`: Column type
  - `"cluster_col": cluster_col`: New column name
  - `"unique_clusters": unique_years`: List of years found
  - `"description": ...`: Human-readable description
- `continue`: Skip to next column (don't process as numeric/categorical)

**Lines 209-220: NUMERIC CLUSTERING**

**Line 210:** Check if column is numeric
- `if pd.api.types.is_numeric_dtype(col_series):`
- True for int, float columns

**Lines 211-213:** Bin numeric values
- `cluster_col = f"{col}__bin" if keep_original else col`: Create column name
- `labels = cluster_numeric_column(col_series, n_bins=numeric_bins)`: Bin values
- `df_proc[cluster_col] = labels`: Add to dataframe

**Lines 215-220:** Store mapping
- Similar to year clustering
- Stores bin labels like "(18.0, 25.0]", "(25.0, 35.0]"

**Lines 222-234: CATEGORICAL CLUSTERING**

**Lines 223-225:** Group rare categories
- Applies to all non-numeric, non-year columns
- `cluster_categorical_column(col_series, top_k=cat_top_k)`: Keep top K

**Lines 227-234:** Store mapping
- Similar structure to previous clusterings

**Line 236:** Return results
- `return df_proc, mappings`: Processed dataframe + metadata

**Example Usage:**
```python
Input DataFrame:
   Year    Age    Department
0  2020    25     Engineering
1  2021    30     Medicine
2  2022    35     Engineering
3  2023    40     Law

Output (df_proc):
   Year  Year__year  Age  Age__bin         Department  Department__cat
0  2020  2020        25   (24.0, 30.0]    Engineering  Engineering
1  2021  2021        30   (24.0, 30.0]    Medicine     Medicine
2  2022  2022        35   (30.0, 40.0]    Engineering  Engineering
3  2023  2023        40   (30.0, 40.0]    Law          Law

Output (mappings):
{
    "Year": {
        "type": "year",
        "cluster_col": "Year__year",
        "unique_clusters": [2020, 2021, 2022, 2023],
        "description": "Year clustering: 4 unique years"
    },
    "Age": {
        "type": "numeric",
        "cluster_col": "Age__bin",
        "unique_clusters": ["(24.0, 30.0]", "(30.0, 40.0]"],
        "description": "Numeric binning: 2 bins"
    },
    "Department": {
        "type": "categorical",
        "cluster_col": "Department__cat",
        "unique_clusters": ["Engineering", "Medicine", "Law"],
        "description": "Categorical grouping: 3 groups"
    }
}
```

---

## 5. Forecasting Functions (Lines 242-380)

### Lines 242-280: simple_forecast_model()
```python
def simple_forecast_model(series: pd.Series, n_lags: int = 3) -> Dict[str, Any]:
    """Simple lag-based forecasting."""
    try:
        s = series.dropna().sort_index()
        if len(s) < 6:
            return {"status": "insufficient_data"}

        # Create lag features
        data = pd.DataFrame({"target": s})
        for i in range(1, n_lags + 1):
            data[f"lag_{i}"] = data["target"].shift(i)

        data = data.dropna()
        if len(data) < 3:
            return {"status": "insufficient_data"}

        X = data[[f"lag_{i}" for i in range(1, n_lags + 1)]]
        y = data["target"]

        split_idx = max(1, len(data) - 2)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        model = LinearRegression()
        model.fit(X_train, y_train)

        if len(X_test) > 0:
            test_preds = model.predict(X_test)
            mse = mean_squared_error(y_test, test_preds)
        else:
            test_preds = []
            mse = 0.0

        # Forecast next period
        last_values = X.iloc[-1].values.reshape(1, -1)
        next_prediction = float(model.predict(last_values)[0])

        return {
            "status": "ok",
            "method": "LAG_REGRESSION",
            "mse": mse,
            "next_period_prediction": next_prediction,
            "periods": data.index.tolist(),
            "y_values": y.tolist(),
            "predictions_test": test_preds.tolist(),
            "actuals_test": y_test.tolist()
        }

    except Exception as e:
        return {"status": "error", "error": str(e)}
```

**FALLBACK FORECASTING - Detailed Explanation:**

**Lines 244-246:** Prepare data
- `s = series.dropna().sort_index()`: Remove NaN, sort by time
- `if len(s) < 6:`: Need at least 6 data points
- Returns error status if insufficient

**Lines 248-250:** Create lag features
- `data = pd.DataFrame({"target": s})`: Create dataframe with target
- `for i in range(1, n_lags + 1):`: Loop 3 times (default)
- `data[f"lag_{i}"] = data["target"].shift(i)`: Create lagged columns

**Lag Feature Example:**
```python
Original series: [100, 105, 110, 115, 120]

After creating lags (n_lags=3):
   target  lag_1  lag_2  lag_3
0    100    NaN    NaN    NaN
1    105  100.0    NaN    NaN
2    110  105.0  100.0    NaN
3    115  110.0  105.0  100.0
4    120  115.0  110.0  105.0
```

**Lines 252-254:** Remove incomplete rows
- `data = data.dropna()`: Remove rows with NaN lags
- `if len(data) < 3:`: Need at least 3 complete rows
- Returns error if insufficient

**Lines 256-257:** Prepare features and target
- `X = data[[f"lag_{i}" for i in range(1, n_lags + 1)]]`: Features (lag columns)
- `y = data["target"]`: Target (actual values)

**Lines 259-261:** Split train/test
- `split_idx = max(1, len(data) - 2)`: Use last 2 points for testing
- `X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]`: Split features
- `y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]`: Split target

**Lines 263-264:** Train model
- `model = LinearRegression()`: Create linear regression model
- `model.fit(X_train, y_train)`: Train on training data

**Model learns:** `target = Î²â‚€ + Î²â‚Ã—lag_1 + Î²â‚‚Ã—lag_2 + Î²â‚ƒÃ—lag_3`

**Lines 266-271:** Evaluate on test set
- `if len(X_test) > 0:`: If test data exists
- `test_preds = model.predict(X_test)`: Make predictions
- `mse = mean_squared_error(y_test, test_preds)`: Calculate error

**Lines 273-275:** Forecast next period
- `last_values = X.iloc[-1].values.reshape(1, -1)`: Get last lag values
- `next_prediction = float(model.predict(last_values)[0])`: Predict next value

**Example:**
```python
Last lags: [120, 115, 110]
Model: target = 5 + 1.0Ã—lag_1 + 0Ã—lag_2 + 0Ã—lag_3
Prediction: 5 + 1.0Ã—120 = 125
```

**Lines 277-286:** Return results
- Dictionary with all forecast information
- Includes method, error metrics, predictions, actuals

**Lines 288-289:** Error handling
- Catches any exceptions
- Returns error status with message

---

### Lines 282-295: aggregate_by_period_for_target()
```python
def aggregate_by_period_for_target(df: pd.DataFrame, date_col: str, target_col: str) -> pd.Series:
    """Aggregate target by year periods."""
    years = round_years_in_column(df[date_col])
    temp_df = pd.DataFrame({
        'year': years,
        'target': pd.to_numeric(df[target_col], errors='coerce')
    }).dropna()

    if len(temp_df) < 4:
        return pd.Series([], dtype=float)

    yearly_data = temp_df.groupby('year')['target'].sum().sort_index()
    return yearly_data
```

**PURPOSE: Convert raw data to time series**

**Line 284:** Extract years from date column
- `years = round_years_in_column(df[date_col])`: Get integer years

**Lines 285-287:** Create temporary dataframe
- `temp_df = pd.DataFrame({...})`: Combine years and target
- `pd.to_numeric(df[target_col], errors='coerce')`: Convert to numbers, NaN if fails
- `.dropna()`: Remove rows with missing data

**Example:**
```python
Input DataFrame:
   Academic_Year  Enrollment
0  2020/21        1000
1  2020/21        500
2  2021/22        1200
3  2021/22        600

After processing:
   year  target
0  2020  1000
1  2020  500
2  2021  1200
3  2021  600
```

**Lines 289-290:** Check sufficient data
- `if len(temp_df) < 4:`: Need at least 4 rows
- Returns empty series if insufficient

**Line 292:** Aggregate by year
- `temp_df.groupby('year')['target'].sum()`: Sum target by year
- `.sort_index()`: Sort by year

**Result:**
```python
year
2020    1500  (1000 + 500)
2021    1800  (1200 + 600)
```

---

### Lines 297-318: round_to_historical_pattern()
```python
def round_to_historical_pattern(prediction: float, historical_data: pd.Series) -> int:
    """Round predictions based on historical patterns"""
    # Get common rounding patterns from historical data
    differences = historical_data.diff().dropna()
    median_change = abs(differences.median())
    
    # Find most common last digits
    last_digits = [int(str(abs(int(x)))[-2:]) for x in historical_data if not pd.isna(x)]
    if last_digits:
        common_last_digits = pd.Series(last_digits).mode()[0]
    else:
        common_last_digits = 0
    
    # Round to nearest multiple of pattern
    if median_change > 100:
        base = 10 ** (len(str(int(median_change))) - 2)
        rounded = round(prediction / base) * base
    else:
        rounded = round(prediction)
    
    # Adjust last two digits to match historical pattern
    rounded_str = str(int(rounded))
    if len(rounded_str) >= 2:
        rounded_str = rounded_str[:-2] + str(common_last_digits).zfill(2)
        rounded = int(rounded_str)
    
    return rounded
```

**PURPOSE: Make predictions look realistic**

**Lines 299-300:** Analyze historical changes
- `differences = historical_data.diff().dropna()`: Calculate year-over-year changes
- `median_change = abs(differences.median())`: Get typical change size

**Example:**
```python
Historical: [1000, 1050, 1100, 1150]
Differences: [50, 50, 50]
Median change: 50
```

**Lines 302-306:** Find common last digits
- `last_digits = [int(str(abs(int(x)))[-2:]) for x in historical_data ...]`
  - Extracts last 2 digits from each value
  - Example: 1050 â†’ "50", 1100 â†’ "00"
- `common_last_digits = pd.Series(last_digits).mode()[0]`: Most common pattern

**Lines 308-312:** Round to appropriate scale
- `if median_change > 100:`: If large changes
  - `base = 10 ** (len(str(int(median_change))) - 2)`: Calculate rounding base
  - Example: median_change=500 â†’ base=10 (round to nearest 10)
- `else:`: Small changes, round to nearest integer

**Lines 314-318:** Adjust last digits
- `rounded_str = rounded_str[:-2] + str(common_last_digits).zfill(2)`
- Replaces last 2 digits with common pattern

**Example:**
```python
Prediction: 1247.3
Historical pattern: values end in "00" or "50"
Rounded: 1250
```

---

*[Continuing with ARIMA and remaining functions...]*



---

## ğŸ¨ Visual Explanations & Alternatives

### Visual 1: Year Extraction Process

```
INPUT STRING: "Academic Year 2023/24"
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  _YEAR_REGEX.search()               â”‚
â”‚  Pattern: \b(19|20)\d{2}\b          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         Finds: "2023"
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  int(match.group(0))                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         OUTPUT: 2023 (integer)

ALTERNATIVES:
1. Manual parsing: year = int(string.split('/')[0])
2. datetime library: pd.to_datetime(string).year
3. Regex with groups: re.search(r'(\d{4})', string)
```

---

### Visual 2: Numeric Binning (qcut vs cut)

```
DATA: [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]

METHOD 1: pd.qcut (Quantile-based - Equal counts)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bin 1       â”‚  Bin 2       â”‚  Bin 3       â”‚
â”‚  (9, 20]     â”‚  (20, 40]    â”‚  (40, 55]    â”‚
â”‚  10,15,20    â”‚  25,30,35,40 â”‚  45,50,55    â”‚
â”‚  3 items     â”‚  4 items     â”‚  3 items     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

METHOD 2: pd.cut (Range-based - Equal widths)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bin 1       â”‚  Bin 2       â”‚  Bin 3       â”‚
â”‚  (9, 24]     â”‚  (24, 39]    â”‚  (39, 55]    â”‚
â”‚  10,15,20    â”‚  25,30,35    â”‚  40,45,50,55 â”‚
â”‚  3 items     â”‚  3 items     â”‚  4 items     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ALTERNATIVES:
1. K-means clustering: from sklearn.cluster import KMeans
2. Custom bins: pd.cut(data, bins=[0, 25, 50, 100])
3. Jenks natural breaks: jenkspy.jenks_breaks()
4. Equal frequency: pd.qcut(data, q=n_bins)
```

---

### Visual 3: Categorical Clustering (Top-K)

```
ORIGINAL DATA (100 rows):
USA: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (40 occurrences)
UK:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (20 occurrences)
CAN: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (15 occurrences)
FRA: â–ˆâ–ˆâ–ˆâ–ˆ (8 occurrences)
GER: â–ˆâ–ˆâ–ˆ (6 occurrences)
ITA: â–ˆâ–ˆ (4 occurrences)
ESP: â–ˆâ–ˆ (3 occurrences)
POR: â–ˆ (2 occurrences)
GRE: â–ˆ (1 occurrence)
SWE: â–ˆ (1 occurrence)

AFTER CLUSTERING (top_k=3):
USA: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (40)
UK:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (20)
CAN: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (15)
Other: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (25)  â† All rare categories grouped

ALTERNATIVES:
1. Frequency threshold: Keep categories with >5% frequency
2. Hierarchical grouping: Group by region (Europe, Asia, etc.)
3. Embedding-based: Group similar categories using word embeddings
4. No grouping: Keep all (risk of overfitting)
```

---

### Visual 4: Lag Feature Creation

```
ORIGINAL TIME SERIES:
Year  Value
2020  100
2021  105
2022  110
2023  115
2024  120

AFTER CREATING LAG FEATURES (n_lags=3):
Year  Value  lag_1  lag_2  lag_3
2020  100    NaN    NaN    NaN    â† Dropped (incomplete)
2021  105    100    NaN    NaN    â† Dropped (incomplete)
2022  110    105    100    NaN    â† Dropped (incomplete)
2023  115    110    105    100    âœ“ Used for training
2024  120    115    110    105    âœ“ Used for training

MODEL LEARNS:
Value = Î²â‚€ + Î²â‚Ã—lag_1 + Î²â‚‚Ã—lag_2 + Î²â‚ƒÃ—lag_3

PREDICTION FOR 2025:
lag_1=120, lag_2=115, lag_3=110
Predicted = Î²â‚€ + Î²â‚Ã—120 + Î²â‚‚Ã—115 + Î²â‚ƒÃ—110 = 125

ALTERNATIVES:
1. Moving average: next = mean(last_n_values)
2. Exponential smoothing: weighted average (recent > old)
3. Seasonal decomposition: trend + seasonality + residual
4. LSTM neural network: learns complex patterns
```

---

### Visual 5: ARIMA Model Selection

```
ARIMA(p, d, q) PARAMETERS:
p = AutoRegressive order (use past values)
d = Differencing order (make stationary)
q = Moving Average order (use past errors)

GRID SEARCH PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try all combinations:               â”‚
â”‚ p âˆˆ {0, 1, 2}                      â”‚
â”‚ d âˆˆ {0, 1} (from stationarity test)â”‚
â”‚ q âˆˆ {0, 1, 2}                      â”‚
â”‚                                     â”‚
â”‚ Total: 3 Ã— 1 Ã— 3 = 9 models        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model    AIC Score                  â”‚
â”‚ (0,1,0)  245.3                      â”‚
â”‚ (1,1,0)  238.7  â† Best (lowest AIC) â”‚
â”‚ (2,1,0)  240.1                      â”‚
â”‚ (0,1,1)  242.5                      â”‚
â”‚ (1,1,1)  239.8                      â”‚
â”‚ ...                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    SELECT BEST MODEL
    ARIMA(1,1,0) with AIC=238.7

ALTERNATIVES:
1. Auto ARIMA: pmdarima.auto_arima() (automatic selection)
2. Prophet: Facebook's forecasting library
3. SARIMA: Seasonal ARIMA for monthly/quarterly data
4. VAR: Vector AutoRegression for multiple time series
```

---

### Visual 6: Batch Forecasting Workflow

```
INPUT DATAFRAME:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Year â”‚ Dept   â”‚ Studentsâ”‚ Revenue    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2020 â”‚ CS     â”‚ 1000    â”‚ 500000     â”‚
â”‚ 2021 â”‚ CS     â”‚ 1050    â”‚ 525000     â”‚
â”‚ 2022 â”‚ CS     â”‚ 1100    â”‚ 550000     â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: DETECT COLUMNS
Time columns: [Year]
Numeric columns: [Students, Revenue]

STEP 2: CREATE COMBINATIONS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Combination 1: Students by Year â”‚
â”‚ Combination 2: Revenue by Year  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: AGGREGATE BY TIME
Students by Year:
2020: 1000
2021: 1050
2022: 1100

Revenue by Year:
2020: 500000
2021: 525000
2022: 550000

STEP 4: TRY ARIMA
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARIMA(1,1,1) for Students        â”‚
â”‚ Status: âœ“ Success                â”‚
â”‚ Prediction: 1150                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 5: FALLBACK IF NEEDED
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If ARIMA fails:                  â”‚
â”‚ â†’ Try Lag Regression             â”‚
â”‚ â†’ If that fails: Skip            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT:
{
  "Students__by__Year": {
    "status": "ok",
    "method": "ARIMA(1,1,1)",
    "next_period_prediction": 1150
  },
  "Revenue__by__Year": {
    "status": "ok",
    "method": "LAG_REGRESSION",
    "next_period_prediction": 575000,
    "fallback": true
  },
  "_metadata": {
    "total_combinations": 2,
    "successful_arima": 1,
    "fallback_regression": 1
  }
}

ALTERNATIVES:
1. User-selected forecasts: Let user choose what to predict
2. Single best forecast: Only return highest confidence
3. Parallel processing: Use multiprocessing for speed
4. Ensemble: Combine multiple forecasting methods
```

---

### Visual 7: RandomForest Training Process

```
INPUT DATA:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age â”‚ GPA â”‚ Dept â”‚ Passed â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 20  â”‚ 3.5 â”‚ CS   â”‚ Yes    â”‚
â”‚ 22  â”‚ 2.8 â”‚ ENG  â”‚ No     â”‚
â”‚ 21  â”‚ 3.9 â”‚ CS   â”‚ Yes    â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: PREPROCESSING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Separate target (Passed)       â”‚
â”‚ â€¢ One-hot encode (Dept)          â”‚
â”‚ â€¢ Handle missing values          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age â”‚ GPA â”‚ Dept_CSâ”‚ Dept_ENGâ”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 20  â”‚ 3.5 â”‚ 1      â”‚ 0       â”‚
â”‚ 22  â”‚ 2.8 â”‚ 0      â”‚ 1       â”‚
â”‚ 21  â”‚ 3.9 â”‚ 1      â”‚ 0       â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: TRAIN/TEST SPLIT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training (70%) â”‚ Testing (30%)â”‚
â”‚ 70 samples     â”‚ 30 samples   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: TRAIN RANDOM FOREST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Random Forest            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Tree 1â”‚ â”‚Tree 2â”‚ â”‚Tree 3â”‚   â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜   â”‚
â”‚     â”‚        â”‚        â”‚         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚            â†“                    â”‚
â”‚      Majority Vote              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 4: EVALUATE
Accuracy: 85%
Predictions: [Yes, No, Yes, Yes, No, ...]
Actuals:     [Yes, No, Yes, No, No, ...]

ALTERNATIVES:
1. XGBoost: Gradient boosting (often more accurate)
   from xgboost import XGBClassifier
   
2. LightGBM: Faster gradient boosting
   from lightgbm import LGBMClassifier
   
3. Neural Network: Deep learning approach
   from tensorflow.keras import Sequential
   
4. Logistic Regression: Simple, interpretable
   from sklearn.linear_model import LogisticRegression
```

---

### Visual 8: Educational Context Detection

```
INPUT DATAFRAME COLUMNS:
["Student_ID", "Mathematics_Score", "Engineering_Dept", "Enrollment_2023"]

DETECTION PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Check column names              â”‚
â”‚ "Mathematics" â†’ Found in k12_subjects   â”‚
â”‚ "Engineering" â†’ Found in engineering    â”‚
â”‚ "Enrollment" â†’ Found in metrics         â”‚
â”‚                                         â”‚
â”‚ Matches: 3 keywords                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Check data values (sample)      â”‚
â”‚ Row 1: "Computer Science"               â”‚
â”‚ Row 2: "Mechanical Engineering"         â”‚
â”‚                                         â”‚
â”‚ Value matches: 2 columns                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RESULT:                                 â”‚
â”‚ is_educational: True                    â”‚
â”‚ confidence: 40 (3Ã—10 + 2Ã—5)            â”‚
â”‚ detected_categories:                    â”‚
â”‚   - k12_subjects: ["mathematics"]      â”‚
â”‚   - engineering: ["engineering"]       â”‚
â”‚   - metrics: ["enrollment"]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ALTERNATIVES:
1. Machine Learning Classifier:
   Train model on labeled datasets
   
2. Rule-based with weights:
   Different keywords have different importance
   
3. NLP-based:
   Use sentence embeddings to detect context
   
4. User input:
   Let user manually specify domain
```

---

### Visual 9: Clustering Workflow (cluster_all_columns)

```
INPUT DATAFRAME:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Year     â”‚ Age â”‚ Department â”‚ Score   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2020/21  â”‚ 25  â”‚ CS         â”‚ 85.5    â”‚
â”‚ 2021/22  â”‚ 30  â”‚ Engineeringâ”‚ 90.2    â”‚
â”‚ 2022/23  â”‚ 35  â”‚ CS         â”‚ 78.9    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PROCESSING EACH COLUMN:

COLUMN 1: "Year"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check: Contains "year"? YES     â”‚
â”‚ Check: Values look like years?  â”‚
â”‚   "2020/21" â†’ Extract 2020      â”‚
â”‚   "2021/22" â†’ Extract 2021      â”‚
â”‚ Action: Create Year__year       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COLUMN 2: "Age"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check: Is numeric? YES          â”‚
â”‚ Action: Bin into ranges         â”‚
â”‚   25 â†’ "(24, 30]"              â”‚
â”‚   30 â†’ "(24, 30]"              â”‚
â”‚   35 â†’ "(30, 35]"              â”‚
â”‚ Create: Age__bin                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COLUMN 3: "Department"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check: Is categorical? YES      â”‚
â”‚ Action: Keep top 15, group rest â”‚
â”‚   CS â†’ CS (top category)        â”‚
â”‚   Engineering â†’ Engineering     â”‚
â”‚ Create: Department__cat         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COLUMN 4: "Score"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check: Is numeric? YES          â”‚
â”‚ Action: Bin into ranges         â”‚
â”‚   85.5 â†’ "(78, 90]"            â”‚
â”‚   90.2 â†’ "(90, 100]"           â”‚
â”‚   78.9 â†’ "(78, 90]"            â”‚
â”‚ Create: Score__bin              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT DATAFRAME:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Year     â”‚Year__year â”‚ Age â”‚ Age__binâ”‚ Department â”‚Department__catâ”‚ Score â”‚Score__binâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2020/21  â”‚ 2020      â”‚ 25  â”‚(24, 30] â”‚ CS         â”‚ CS            â”‚ 85.5  â”‚(78, 90]  â”‚
â”‚ 2021/22  â”‚ 2021      â”‚ 30  â”‚(24, 30] â”‚ Engineeringâ”‚ Engineering   â”‚ 90.2  â”‚(90, 100] â”‚
â”‚ 2022/23  â”‚ 2022      â”‚ 35  â”‚(30, 35] â”‚ CS         â”‚ CS            â”‚ 78.9  â”‚(78, 90]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MAPPINGS OUTPUT:
{
  "Year": {
    "type": "year",
    "cluster_col": "Year__year",
    "unique_clusters": [2020, 2021, 2022],
    "description": "Year clustering: 3 unique years"
  },
  "Age": {
    "type": "numeric",
    "cluster_col": "Age__bin",
    "unique_clusters": ["(24, 30]", "(30, 35]"],
    "description": "Numeric binning: 2 bins"
  },
  ...
}

ALTERNATIVES:
1. No clustering: Use raw values (risk of overfitting)
2. Manual binning: User defines bin ranges
3. Adaptive binning: Adjust bins based on distribution
4. ML-based: Use clustering algorithms (K-means, DBSCAN)
```

---

### Visual 10: ARIMA vs Lag Regression Comparison

```
SCENARIO: Forecasting student enrollment

DATA: [1000, 1050, 1100, 1150, 1200]

METHOD 1: ARIMA(1,1,1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Advantages:                         â”‚
â”‚ âœ“ Handles trends automatically      â”‚
â”‚ âœ“ Statistical foundation            â”‚
â”‚ âœ“ Confidence intervals              â”‚
â”‚ âœ“ Works with irregular data         â”‚
â”‚                                     â”‚
â”‚ Disadvantages:                      â”‚
â”‚ âœ— Can fail to converge              â”‚
â”‚ âœ— Requires stationarity testing     â”‚
â”‚ âœ— Slower computation                â”‚
â”‚ âœ— Sensitive to outliers             â”‚
â”‚                                     â”‚
â”‚ Prediction: 1250 Â± 50               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

METHOD 2: Lag Regression
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Advantages:                         â”‚
â”‚ âœ“ Always converges                  â”‚
â”‚ âœ“ Fast computation                  â”‚
â”‚ âœ“ Simple to understand              â”‚
â”‚ âœ“ Robust to outliers                â”‚
â”‚                                     â”‚
â”‚ Disadvantages:                      â”‚
â”‚ âœ— No confidence intervals           â”‚
â”‚ âœ— Assumes linear relationship       â”‚
â”‚ âœ— Loses data to lag creation        â”‚
â”‚ âœ— Less sophisticated                â”‚
â”‚                                     â”‚
â”‚ Prediction: 1245                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DECISION TREE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try ARIMA first     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ Converged?  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Yes â†’ Use ARIMA result  â”‚
    â”‚ No  â†’ Fallback to Lag   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OTHER ALTERNATIVES:
1. Prophet: Facebook's forecasting
   - Handles seasonality
   - Automatic holiday detection
   
2. Exponential Smoothing:
   - Simple weighted average
   - Good for short-term forecasts
   
3. LSTM Neural Network:
   - Deep learning approach
   - Needs more data (100+ points)
   
4. Ensemble:
   - Combine multiple methods
   - Average predictions
```

---

### Visual 11: Data Flow Through System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER UPLOADS CSV                         â”‚
â”‚              "student_data_2020_2023.csv"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 1: LOAD & VALIDATE                        â”‚
â”‚  â€¢ Check file format                                        â”‚
â”‚  â€¢ Load into pandas DataFrame                               â”‚
â”‚  â€¢ Display preview                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STEP 2: DETECT EDUCATIONAL CONTEXT                  â”‚
â”‚  â€¢ Scan column names for keywords                           â”‚
â”‚  â€¢ Check data values                                        â”‚
â”‚  â€¢ Return confidence score                                  â”‚
â”‚  Result: "Educational dataset detected (85% confidence)"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            STEP 3: AUTO-DETECT COLUMN TYPES                 â”‚
â”‚  Year columns: ["Academic_Year"]                            â”‚
â”‚  Numeric columns: ["Enrollment", "GPA", "Age"]              â”‚
â”‚  Categorical columns: ["Department", "Status"]              â”‚
â”‚  Educational columns: ["Enrollment", "Department"]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 4: CLUSTER ALL COLUMNS                    â”‚
â”‚  â€¢ Year extraction: "2020/21" â†’ 2020                        â”‚
â”‚  â€¢ Numeric binning: Age â†’ "(18-25]", "(25-35]"             â”‚
â”‚  â€¢ Categorical grouping: Keep top 15, rest â†’ "Other"       â”‚
â”‚  Result: Original + Clustered columns                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           STEP 5: BATCH FORECASTING                         â”‚
â”‚  For each (time_col, numeric_col) pair:                    â”‚
â”‚    1. Aggregate by time period                              â”‚
â”‚    2. Try ARIMA forecasting                                 â”‚
â”‚    3. If fails, try Lag Regression                          â”‚
â”‚    4. Store result with metadata                            â”‚
â”‚  Result: Dictionary of forecasts                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 6: BUILD RAG SYSTEM                       â”‚
â”‚  â€¢ Convert CSV rows to documents                            â”‚
â”‚  â€¢ Generate embeddings (768-dim vectors)                    â”‚
â”‚  â€¢ Store in FAISS vector database                           â”‚
â”‚  â€¢ Create retriever for Q&A                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 7: USER INTERACTION                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ View Data    â”‚ Visualize    â”‚ Ask Questionsâ”‚            â”‚
â”‚  â”‚ â€¢ Preview    â”‚ â€¢ Charts     â”‚ â€¢ AI Q&A     â”‚            â”‚
â”‚  â”‚ â€¢ Statistics â”‚ â€¢ PyGwalker  â”‚ â€¢ Forecasts  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Visual 12: Error Handling Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Function: arima_forecast_students  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check: len(series) >= 4?            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ No      â”‚ Yes
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Return â”‚  â”‚ Try: adfuller test   â”‚
â”‚Error  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Success?       â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚ No      â”‚ Yes
         â”‚         â”‚
         â–¼         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Set    â”‚  â”‚ Use result   â”‚
    â”‚ d=1    â”‚  â”‚ d=0 or d=1   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Try: Fit ARIMA models â”‚
    â”‚ Loop: p=0,1,2 q=0,1,2 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Any success?  â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
    â”‚ No    â”‚ Yes
    â”‚       â”‚
    â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Return  â”‚ â”‚ Make forecastâ”‚
â”‚Error   â”‚ â”‚ Return resultâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GRACEFUL DEGRADATION:
Level 1: ARIMA âœ“
Level 2: Lag Regression (fallback)
Level 3: Return error with details

ALTERNATIVES:
1. Fail fast: Stop on first error
2. Retry with different parameters
3. Use default simple forecast
4. Ask user for input
```

---

## ğŸ”„ Complete Alternatives Summary

### For Year Extraction:
| Method | Pros | Cons | Use When |
|--------|------|------|----------|
| Regex (current) | Fast, flexible | May miss edge cases | General purpose |
| datetime parsing | Handles formats | Slower, strict | Known date formats |
| Manual split | Simple | Fragile | Fixed format only |
| NLP extraction | Intelligent | Overkill, slow | Complex text |

### For Clustering:
| Method | Pros | Cons | Use When |
|--------|------|------|----------|
| Quantile binning | Equal counts | Unequal widths | Skewed data |
| Range binning | Equal widths | Unequal counts | Uniform data |
| K-means | Data-driven | Needs tuning | Complex patterns |
| Manual bins | Domain knowledge | Not adaptive | Known ranges |

### For Forecasting:
| Method | Pros | Cons | Use When |
|--------|------|------|----------|
| ARIMA | Statistical, CI | Can fail | Stationary data |
| Lag Regression | Always works | Simple | Quick forecast |
| Prophet | Handles seasonality | Slower | Monthly/yearly |
| LSTM | Complex patterns | Needs data | 100+ points |

### For ML Models:
| Method | Pros | Cons | Use When |
|--------|------|------|----------|
| RandomForest | Robust, fast | Black box | General purpose |
| XGBoost | More accurate | Slower | Competitions |
| LightGBM | Very fast | Less accurate | Large data |
| Linear | Interpretable | Too simple | Simple patterns |

---

**Summary:** This comprehensive visual guide shows how every function works, what alternatives exist, and when to use each approach. The visuals make complex concepts easy to understand for developers at all levels.

